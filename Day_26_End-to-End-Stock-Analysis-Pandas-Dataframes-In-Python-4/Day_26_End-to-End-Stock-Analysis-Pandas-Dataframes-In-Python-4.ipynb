{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B881F9xu4Bmk"
      },
      "source": [
        "### Important DataFrame Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd985cwtJnCW",
        "outputId": "38fd851c-a06f-474d-dd59-5e95b369b1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting numpy>=1.26.0 (from pandas)\n",
            "  Downloading numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
            "Successfully installed numpy-2.0.0 pandas-2.2.2 tzdata-2024.1\n"
          ]
        }
      ],
      "source": [
        "# astype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "76Gugqiz4F8s"
      },
      "outputs": [],
      "source": [
        "# value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Identify all the top-performing stocks based on their overall return from the start to the end of the dataset.**\n",
        "   - **Hint:** Calculate the return as (final close price - initial close price) / initial close price.\n",
        "\n",
        "2. **How many instances of daily price change greater than 5% (super over finishes) occurred for each stock?**\n",
        "   - **Hint:** Use the `pct_change()` method on the 'Close' price to calculate daily percentage changes.\n",
        "\n",
        "3. **Calculate the number of times each stock's price increased on a specific date range (e.g., within the first quarter of each year).**\n",
        "   - **Hint:** Filter the dataset by date and then compare 'Close' and 'Open' prices.\n",
        "\n",
        "4. **Calculate the percentage of times the opening price being higher than the previous day's closing price (toss winner) resulted in a higher closing price on that day (match winner).**\n",
        "   - **Hint:** Use the `shift()` method to compare the opening price with the previous day's closing price.\n",
        "\n",
        "5. **Identify all assets with a daily percentage change higher than 8% and a volume greater than 100,000.**\n",
        "   - **Hint:** Apply conditions on 'Daily Change' and 'Volume' columns.\n",
        "\n",
        "6. **Identify all stocks in the Technology sector with a daily percentage change higher than 7.5%.**\n",
        "   - **Hint:** Merge the datasets on 'Ticker' and filter based on the sector and daily change.\n",
        "\n",
        "7. **Write a function that can return the performance track record of two assets against each other over time.**\n",
        "   - **Hint:** Create a function that merges the close prices of two tickers on the 'Date' column.\n",
        "\n",
        "8. **Identify and remove duplicate rows from both datasets. How many duplicates were removed?**\n",
        "   - **Hint:** Use the `duplicated()` method to find and remove duplicate rows.\n",
        "\n",
        "9. **For the fundamental dataset, identify rows where the Market Cap is NaN, and impute these values using a sector-wise median Market Cap.**\n",
        "   - **Hint:** Group by sector and fill NaN values with the sector median.\n",
        "\n",
        "10. **Filter the combined dataset to find the top 5 most volatile stocks based on the standard deviation of daily percentage changes.**\n",
        "    - **Hint:** Calculate the standard deviation of daily percentage changes for each stock and sort the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset created and saved to 'algotrading_combined_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to create a synthetic dataset for a given asset type\n",
        "def create_asset_data(tickers, start_date, end_date):\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "    data = []\n",
        "    for ticker in tickers:\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        price = np.random.randn(len(dates)).cumsum() + 100\n",
        "        for i in range(len(dates)):\n",
        "            open_price = price[i] + np.random.uniform(-1, 1)\n",
        "            high_price = max(open_price, price[i] + np.random.uniform(0, 2))\n",
        "            low_price = min(open_price, price[i] - np.random.uniform(0, 2))\n",
        "            close_price = price[i] + np.random.uniform(-1, 1)\n",
        "            volume = np.random.randint(1000, 1000000)\n",
        "            # Introduce NaN values randomly\n",
        "            if np.random.rand() < 0.05:\n",
        "                open_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                high_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                low_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                close_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                volume = np.nan\n",
        "            data.append([dates[i], ticker, open_price, high_price, low_price, close_price, volume])\n",
        "    return pd.DataFrame(data, columns=['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "# Define tickers for each asset type\n",
        "stock_tickers = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'TSLA']\n",
        "forex_tickers = ['EUR/USD', 'GBP/USD', 'USD/JPY', 'AUD/USD', 'USD/CAD']\n",
        "crypto_tickers = ['BTC/USD', 'ETH/USD', 'XRP/USD', 'LTC/USD', 'BCH/USD']\n",
        "\n",
        "# Generate data for each asset type\n",
        "stock_data = create_asset_data(stock_tickers, '2015-01-01', '2023-01-01')\n",
        "forex_data = create_asset_data(forex_tickers, '2015-01-01', '2023-01-01')\n",
        "crypto_data = create_asset_data(crypto_tickers, '2015-01-01', '2023-01-01')\n",
        "\n",
        "# Combine all data into a single DataFrame\n",
        "combined_data = pd.concat([stock_data, forex_data, crypto_data])\n",
        "\n",
        "# Introduce duplicate rows\n",
        "combined_data = pd.concat([combined_data, combined_data.sample(frac=0.05, random_state=42)])  # Add 5% duplicates\n",
        "\n",
        "# Save to CSV\n",
        "combined_data.to_csv('algotrading_combined_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset created and saved to 'algotrading_combined_dataset.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset created and saved to 'fundamental_research_dataset_large.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of real-life stock tickers and company names (a sample list)\n",
        "tickers_and_companies = [\n",
        "    ('AAPL', 'Apple Inc.'),\n",
        "    ('GOOGL', 'Alphabet Inc.'),\n",
        "    ('MSFT', 'Microsoft Corporation'),\n",
        "    ('AMZN', 'Amazon.com, Inc.'),\n",
        "    ('TSLA', 'Tesla, Inc.'),\n",
        "    ('FB', 'Meta Platforms, Inc.'),\n",
        "    ('BRK.B', 'Berkshire Hathaway Inc.'),\n",
        "    ('JNJ', 'Johnson & Johnson'),\n",
        "    ('JPM', 'JPMorgan Chase & Co.'),\n",
        "    ('V', 'Visa Inc.'),\n",
        "    # Add more real-life tickers and company names to reach at least 1000 companies\n",
        "]\n",
        "\n",
        "# Extend the list to include at least 1000 companies\n",
        "while len(tickers_and_companies) < 1000:\n",
        "    tickers_and_companies.extend(tickers_and_companies[:1000 - len(tickers_and_companies)])\n",
        "\n",
        "# Define sectors and industries\n",
        "sectors = ['Technology', 'Healthcare', 'Finance', 'Consumer Goods', 'Utilities', 'Energy', 'Industrial Goods']\n",
        "industries = {\n",
        "    'Technology': ['Software', 'Hardware', 'Semiconductors'],\n",
        "    'Healthcare': ['Pharmaceuticals', 'Biotechnology', 'Medical Devices'],\n",
        "    'Finance': ['Banks', 'Insurance', 'Investment Services'],\n",
        "    'Consumer Goods': ['Beverages', 'Food Products', 'Household Products'],\n",
        "    'Utilities': ['Electric Utilities', 'Gas Utilities', 'Water Utilities'],\n",
        "    'Energy': ['Oil & Gas', 'Renewable Energy', 'Coal'],\n",
        "    'Industrial Goods': ['Aerospace', 'Construction', 'Manufacturing']\n",
        "}\n",
        "\n",
        "# Generate random data for each ticker\n",
        "data = []\n",
        "for ticker, company_name in tickers_and_companies:\n",
        "    sector = np.random.choice(sectors)\n",
        "    industry = np.random.choice(industries[sector])\n",
        "    market_cap = round(np.random.uniform(0.1, 500), 2)  # in billions\n",
        "    eps = round(np.random.uniform(-10, 10), 2)\n",
        "    pe_ratio = round(np.random.uniform(5, 50), 2)\n",
        "    dividend_yield = round(np.random.uniform(0, 10), 2)\n",
        "    pb_ratio = round(np.random.uniform(0.1, 20), 2)\n",
        "    de_ratio = round(np.random.uniform(0, 3), 2)\n",
        "\n",
        "    # Introduce NaN values randomly\n",
        "    if np.random.rand() < 0.1: market_cap = np.nan\n",
        "    if np.random.rand() < 0.1: eps = np.nan\n",
        "    if np.random.rand() < 0.1: pe_ratio = np.nan\n",
        "    if np.random.rand() < 0.1: dividend_yield = np.nan\n",
        "    if np.random.rand() < 0.1: pb_ratio = np.nan\n",
        "    if np.random.rand() < 0.1: de_ratio = np.nan\n",
        "\n",
        "    data.append([ticker, company_name, sector, industry, market_cap, eps, pe_ratio, dividend_yield, pb_ratio, de_ratio])\n",
        "\n",
        "# Convert to DataFrame\n",
        "columns = ['Ticker', 'Company Name', 'Sector', 'Industry', 'Market Cap (B)', 'EPS', 'P/E Ratio', 'Dividend Yield (%)', 'P/B Ratio', 'D/E Ratio']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Introduce duplicate rows\n",
        "df = pd.concat([df, df.sample(frac=0.05, random_state=42)])  # Add 5% duplicates\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('fundamental_research_dataset_large.csv', index=False)\n",
        "\n",
        "print(\"Dataset created and saved to 'fundamental_research_dataset_large.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Dataset 1: Fundamental Research Dataset\n",
        "\n",
        "This dataset contains fundamental data for real-life companies, with some rows containing NaN values and duplicates.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of real-life stock tickers and company names (a sample list)\n",
        "tickers_and_companies = [\n",
        "    ('AAPL', 'Apple Inc.'),\n",
        "    ('GOOGL', 'Alphabet Inc.'),\n",
        "    ('MSFT', 'Microsoft Corporation'),\n",
        "    ('AMZN', 'Amazon.com, Inc.'),\n",
        "    ('TSLA', 'Tesla, Inc.'),\n",
        "    ('FB', 'Meta Platforms, Inc.'),\n",
        "    ('BRK.B', 'Berkshire Hathaway Inc.'),\n",
        "    ('JNJ', 'Johnson & Johnson'),\n",
        "    ('JPM', 'JPMorgan Chase & Co.'),\n",
        "    ('V', 'Visa Inc.'),\n",
        "    # Add more real-life tickers and company names to reach at least 1000 companies\n",
        "]\n",
        "\n",
        "# Extend the list to include at least 1000 companies\n",
        "while len(tickers_and_companies) < 1000:\n",
        "    tickers_and_companies.extend(tickers_and_companies[:1000 - len(tickers_and_companies)])\n",
        "\n",
        "# Define sectors and industries\n",
        "sectors = ['Technology', 'Healthcare', 'Finance', 'Consumer Goods', 'Utilities', 'Energy', 'Industrial Goods']\n",
        "industries = {\n",
        "    'Technology': ['Software', 'Hardware', 'Semiconductors'],\n",
        "    'Healthcare': ['Pharmaceuticals', 'Biotechnology', 'Medical Devices'],\n",
        "    'Finance': ['Banks', 'Insurance', 'Investment Services'],\n",
        "    'Consumer Goods': ['Beverages', 'Food Products', 'Household Products'],\n",
        "    'Utilities': ['Electric Utilities', 'Gas Utilities', 'Water Utilities'],\n",
        "    'Energy': ['Oil & Gas', 'Renewable Energy', 'Coal'],\n",
        "    'Industrial Goods': ['Aerospace', 'Construction', 'Manufacturing']\n",
        "}\n",
        "\n",
        "# Generate random data for each ticker\n",
        "data = []\n",
        "for ticker, company_name in tickers_and_companies:\n",
        "    sector = np.random.choice(sectors)\n",
        "    industry = np.random.choice(industries[sector])\n",
        "    market_cap = round(np.random.uniform(0.1, 500), 2)  # in billions\n",
        "    eps = round(np.random.uniform(-10, 10), 2)\n",
        "    pe_ratio = round(np.random.uniform(5, 50), 2)\n",
        "    dividend_yield = round(np.random.uniform(0, 10), 2)\n",
        "    pb_ratio = round(np.random.uniform(0.1, 20), 2)\n",
        "    de_ratio = round(np.random.uniform(0, 3), 2)\n",
        "\n",
        "    # Introduce NaN values randomly\n",
        "    if np.random.rand() < 0.1: market_cap = np.nan\n",
        "    if np.random.rand() < 0.1: eps = np.nan\n",
        "    if np.random.rand() < 0.1: pe_ratio = np.nan\n",
        "    if np.random.rand() < 0.1: dividend_yield = np.nan\n",
        "    if np.random.rand() < 0.1: pb_ratio = np.nan\n",
        "    if np.random.rand() < 0.1: de_ratio = np.nan\n",
        "\n",
        "    data.append([ticker, company_name, sector, industry, market_cap, eps, pe_ratio, dividend_yield, pb_ratio, de_ratio])\n",
        "\n",
        "# Convert to DataFrame\n",
        "columns = ['Ticker', 'Company Name', 'Sector', 'Industry', 'Market Cap (B)', 'EPS', 'P/E Ratio', 'Dividend Yield (%)', 'P/B Ratio', 'D/E Ratio']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Introduce duplicate rows\n",
        "df = pd.concat([df, df.sample(frac=0.05, random_state=42)])  # Add 5% duplicates\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('fundamental_research_dataset_large.csv', index=False)\n",
        "\n",
        "print(\"Dataset created and saved to 'fundamental_research_dataset_large.csv'\")\n",
        "```\n",
        "\n",
        "### Dataset 2: Combined Asset Data\n",
        "\n",
        "This dataset includes stock, Forex, and cryptocurrency data with NaN values and duplicates.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to create a synthetic dataset for a given asset type\n",
        "def create_asset_data(tickers, start_date, end_date):\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "    data = []\n",
        "    for ticker in tickers:\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        price = np.random.randn(len(dates)).cumsum() + 100\n",
        "        for i in range(len(dates)):\n",
        "            open_price = price[i] + np.random.uniform(-1, 1)\n",
        "            high_price = max(open_price, price[i] + np.random.uniform(0, 2))\n",
        "            low_price = min(open_price, price[i] - np.random.uniform(0, 2))\n",
        "            close_price = price[i] + np.random.uniform(-1, 1)\n",
        "            volume = np.random.randint(1000, 1000000)\n",
        "            # Introduce NaN values randomly\n",
        "            if np.random.rand() < 0.05:\n",
        "                open_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                high_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                low_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                close_price = np.nan\n",
        "            if np.random.rand() < 0.05:\n",
        "                volume = np.nan\n",
        "            data.append([dates[i], ticker, open_price, high_price, low_price, close_price, volume])\n",
        "    return pd.DataFrame(data, columns=['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "# Define tickers for each asset type\n",
        "stock_tickers = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'TSLA']\n",
        "forex_tickers = ['EUR/USD', 'GBP/USD', 'USD/JPY', 'AUD/USD', 'USD/CAD']\n",
        "crypto_tickers = ['BTC/USD', 'ETH/USD', 'XRP/USD', 'LTC/USD', 'BCH/USD']\n",
        "\n",
        "# Generate data for each asset type\n",
        "stock_data = create_asset_data(stock_tickers, '2015-01-01', '2023-01-01')\n",
        "forex_data = create_asset_data(forex_tickers, '2015-01-01', '2023-01-01')\n",
        "crypto_data = create_asset_data(crypto_tickers, '2015-01-01', '2023-01-01')\n",
        "\n",
        "# Combine all data into a single DataFrame\n",
        "combined_data = pd.concat([stock_data, forex_data, crypto_data])\n",
        "\n",
        "# Introduce duplicate rows\n",
        "combined_data = pd.concat([combined_data, combined_data.sample(frac=0.05, random_state=42)])  # Add 5% duplicates\n",
        "\n",
        "# Save to CSV\n",
        "combined_data.to_csv('algotrading_combined_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset created and saved to 'algotrading_combined_dataset.csv'\")\n",
        "```\n",
        "\n",
        "### Practice Questions\n",
        "\n",
        "1. **Data Cleaning**:\n",
        "   - Identify and remove duplicate rows from both datasets.\n",
        "   - Handle NaN values appropriately (e.g., fill with mean/median, forward fill, backward fill, or remove).\n",
        "\n",
        "2. **Data Analysis**:\n",
        "   - Calculate the average Market Cap, P/E Ratio, and Dividend Yield for each sector in the fundamental research dataset.\n",
        "   - Calculate the daily percentage change for each asset in the combined dataset.\n",
        "\n",
        "3. **Data Visualization**:\n",
        "   - Plot the distribution of Market Cap values for different sectors.\n",
        "   - Visualize the price trends for a selected stock, Forex pair, and cryptocurrency over time.\n",
        "\n",
        "4. **Advanced Analysis**:\n",
        "   - Perform a sector-wise analysis to find which sectors have the highest and lowest average EPS in the fundamental research dataset.\n",
        "   - Analyze the correlation between different Forex pairs in the combined dataset.\n",
        "\n",
        "5. **Algorithmic Trading**:\n",
        "   - Implement a simple moving average crossover strategy using the combined dataset.\n",
        "   - Develop a fundamental analysis-based scoring system to rank stocks in the fundamental research dataset based on their financial metrics.\n",
        "\n",
        "These questions and tasks should help you get hands-on experience with real-life algorithmic trading scenarios and data handling techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 1. Identify all the top-performing stocks based on their overall return from the start to the end of the dataset.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the combined dataset\n",
        "combined_df = pd.read_csv('algotrading_combined_dataset.csv')\n",
        "\n",
        "# Calculate overall return for each asset\n",
        "tickers = combined_df['Ticker'].unique()\n",
        "returns = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    stock_data = combined_df[combined_df['Ticker'] == ticker]\n",
        "    overall_return = (stock_data['Close'].iloc[-1] - stock_data['Close'].iloc[0]) / stock_data['Close'].iloc[0]\n",
        "    returns.append({'Ticker': ticker, 'Overall Return': overall_return})\n",
        "\n",
        "returns_df = pd.DataFrame(returns).sort_values('Overall Return', ascending=False)\n",
        "\n",
        "# Plot the top 10 performing stocks\n",
        "top_10_performers = returns_df.head(10).set_index('Ticker')\n",
        "top_10_performers.plot(kind='bar')\n",
        "plt.title('Top 10 Performing Stocks')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Overall Return')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Top-performing stocks:\\n{returns_df}\")\n",
        "```\n",
        "\n",
        "### 2. How many instances of daily price change greater than 5% occurred for each stock?\n",
        "\n",
        "```python\n",
        "# Calculate daily percentage change\n",
        "combined_df['Daily Change'] = combined_df['Close'].pct_change()\n",
        "\n",
        "# Calculate instances with daily percentage change greater than 5%\n",
        "combined_df['High Change'] = combined_df['Daily Change'] > 0.05\n",
        "\n",
        "# Create a dictionary to count occurrences for each ticker\n",
        "occurrences = {}\n",
        "tickers = combined_df['Ticker'].unique()\n",
        "\n",
        "for ticker in tickers:\n",
        "    occurrences[ticker] = combined_df[(combined_df['Ticker'] == ticker) & (combined_df['High Change'])].shape[0]\n",
        "\n",
        "# Plot the top 10 tickers with most high changes\n",
        "high_change_counts = pd.Series(occurrences).sort_values(ascending=False).head(10)\n",
        "high_change_counts.plot(kind='bar')\n",
        "plt.title('Top 10 Tickers with Daily Price Change > 5%')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Count of High Changes')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Number of instances with daily price change greater than 5%:\\n{occurrences}\")\n",
        "```\n",
        "\n",
        "### 3. Calculate the number of times each stock's price increased in the first quarter.\n",
        "\n",
        "```python\n",
        "# Filter for the first quarter\n",
        "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "first_quarter = combined_df[combined_df['Date'].dt.month.isin([1, 2, 3])]\n",
        "\n",
        "# Count the number of times the stock's price increased\n",
        "first_quarter['Price Increase'] = first_quarter['Close'] > first_quarter['Open']\n",
        "price_increases = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    price_increases[ticker] = first_quarter[(first_quarter['Ticker'] == ticker) & (first_quarter['Price Increase'])].shape[0]\n",
        "\n",
        "# Plot the top 10 stocks with most price increases in the first quarter\n",
        "price_increases_counts = pd.Series(price_increases).sort_values(ascending=False).head(10)\n",
        "price_increases_counts.plot(kind='bar')\n",
        "plt.title('Top 10 Stocks with Price Increases in Q1')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Count of Price Increases')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Number of price increases in the first quarter:\\n{price_increases}\")\n",
        "```\n",
        "\n",
        "### 4. Calculate the percentage of times the opening price being higher than the previous day's closing price resulted in a higher closing price on that day.\n",
        "\n",
        "```python\n",
        "# Sort the DataFrame by Ticker and Date\n",
        "combined_df = combined_df.sort_values(['Ticker', 'Date'])\n",
        "\n",
        "# Calculate if opening price is higher than the previous day's closing price\n",
        "combined_df['Previous Close'] = combined_df['Close'].shift(1)\n",
        "combined_df['Open Higher'] = combined_df['Open'] > combined_df['Previous Close']\n",
        "\n",
        "# Calculate if the closing price is higher than the opening price\n",
        "combined_df['Close Higher'] = combined_df['Close'] > combined_df['Open']\n",
        "\n",
        "# Calculate percentage of open higher being close higher\n",
        "open_close_higher = (combined_df['Open Higher'] & combined_df['Close Higher']).sum()\n",
        "total_open_higher = combined_df['Open Higher'].sum()\n",
        "percentage_open_close_higher = (open_close_higher / total_open_higher) * 100\n",
        "\n",
        "print(f\"Percentage of open higher being close higher: {percentage_open_close_higher:.2f}%\")\n",
        "```\n",
        "\n",
        "### 5. Identify all assets with a daily percentage change higher than 8% and a volume greater than 100,000.\n",
        "\n",
        "```python\n",
        "# Filter for daily percentage change > 8% and volume > 100,000\n",
        "high_change_high_volume = combined_df[(combined_df['Daily Change'] > 0.08) & (combined_df['Volume'] > 100000)]\n",
        "\n",
        "# Plot the count of such instances for each ticker\n",
        "high_change_high_volume_counts = high_change_high_volume['Ticker'].value_counts().head(10)\n",
        "high_change_high_volume_counts.plot(kind='bar')\n",
        "plt.title('Top 10 Tickers with Daily Change > 8% and Volume > 100,000')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Assets with daily percentage change > 8% and volume > 100,000:\\n{high_change_high_volume_counts}\")\n",
        "```\n",
        "\n",
        "### 6. Identify all stocks in the Technology sector with a daily percentage change higher than 7.5%.\n",
        "\n",
        "```python\n",
        "# Load the fundamental dataset\n",
        "fundamental_df = pd.read_csv('fundamental_research_dataset_large.csv')\n",
        "\n",
        "# Merge with combined dataset to get sector information\n",
        "combined_with_sector = combined_df.merge(fundamental_df[['Ticker', 'Sector']], on='Ticker')\n",
        "\n",
        "# Filter for Technology sector and daily percentage change > 7.5%\n",
        "tech_high_change = combined_with_sector[(combined_with_sector['Sector'] == 'Technology') & (combined_with_sector['Daily Change'] > 0.075)]\n",
        "\n",
        "# Plot the count of such instances for each ticker in Technology sector\n",
        "tech_high_change_counts = tech_high_change['Ticker'].value_counts().head(10)\n",
        "tech_high_change_counts.plot(kind='bar')\n",
        "plt.title('Top 10 Technology Stocks with Daily Change > 7.5%')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Technology stocks with daily percentage change > 7.5%:\\n{tech_high_change_counts}\")\n",
        "```\n",
        "\n",
        "### 7. Write a function that can return the performance track record of two assets against each other over time.\n",
        "\n",
        "```python\n",
        "def track_record(asset1, asset2, combined_df):\n",
        "    asset1_data = combined_df[combined_df['Ticker'] == asset1][['Date', 'Close']].rename(columns={'Close': asset1})\n",
        "    asset2_data = combined_df[combined_df['Ticker'] == asset2][['Date', 'Close']].rename(columns={'Close': asset2})\n",
        "\n",
        "    merged_data = pd.merge(asset1_data, asset2_data, on='Date', how='inner')\n",
        "    merged_data.set_index('Date', inplace=True)\n",
        "    \n",
        "    return merged_data\n",
        "\n",
        "# Example usage\n",
        "asset1 = 'AAPL'\n",
        "asset2 = 'GOOGL'\n",
        "track_record_data = track_record(asset1, asset2, combined_df)\n",
        "\n",
        "# Plot the performance track record\n",
        "track_record_data.plot()\n",
        "plt.title(f'Performance Track Record: {asset1} vs {asset2}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Track record of {asset1} against {asset2}:\\n{track_record_data}\")\n",
        "```\n",
        "\n",
        "### 8. Identify and remove duplicate rows from both datasets. How many duplicates were removed?\n",
        "\n",
        "```python\n",
        "# Identify and remove duplicates\n",
        "fundamental_duplicates = fundamental_df.duplicated().sum()\n",
        "combined_duplicates = combined_df.duplicated().sum()\n",
        "\n",
        "fundamental_df_clean = fundamental_df.drop_duplicates()\n",
        "combined_df_clean = combined_df.drop_duplicates()\n",
        "\n",
        "print(f\"Removed {fundamental_duplicates} duplicate rows from the fundamental dataset.\")\n",
        "print(f\"Removed {combined_duplicates} duplicate rows from the combined dataset.\")\n",
        "```\n",
        "\n",
        "### 9. For the fundamental dataset, identify rows where the Market Cap is NaN, and impute these values using a sector-wise median Market Cap.\n",
        "\n",
        "```python\n",
        "# Impute NaN Market Cap values using sector-wise median\n",
        "sectors = fundamental_df['Sector'].unique()\n",
        "\n",
        "for sector in sectors:\n",
        "    sector_median = fundamental_df[fundamental_df['Sector'] == sector]['Market Cap (B)'].median()\n",
        "    fundamental_df.loc[(fundamental_df['Sector'] == sector) & (fundamental_df['Market Cap (B)'].isna()), 'Market Cap (B)'] = sector_median\n",
        "\n",
        "print(f\"Fundamental dataset after imputing NaN Market Cap values:\\n{fundamental_df}\")\n",
        "```\n",
        "\n",
        "### 10. Filter the combined dataset to find the top 5 most volatile stocks based on the standard deviation of daily percentage changes.\n",
        "\n",
        "```python\n",
        "# Calculate daily percentage change\n",
        "combined_df['Daily Change'] = combined_df['Close'].pct_change()\n",
        "\n",
        "# Calculate standard deviation of daily percentage changes for each stock\n",
        "tickers = combined_df['Ticker'].unique()\n",
        "volatilities = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    stock_data = combined_df[combined_df['Ticker'] == ticker]\n",
        "    volatility = stock_data['Daily Change'].std()\n",
        "    volatilities.append({'Ticker': ticker, 'Volatility': volatility})\n",
        "\n",
        "volatility_df = pd.DataFrame(volatilities).sort_values('Volatility', ascending=False).head(5)\n",
        "\n",
        "# Plot the top 5 most volatile stocks\n",
        "volatility_df.set_index('Ticker').plot(kind='bar')\n",
        "plt.title('Top 5 Most Volatile Stocks')\n",
        "plt.xlabel('Ticker')\n",
        "plt.ylabel('Volatility (Standard Deviation of Daily Changes)')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Top 5 most volatile stocks:\\n{volatility_df}\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
